#
# Example of configuration for running a Focused Crawl
#

# Store pages classified as irrelevant pages by the target page classifier
target_storage.store_negative_pages: true

# Limit the max number of pages crawled per domain, in order to avoid crawling
# too many pages from same somain and favor discovery o new domains
link_storage.max_pages_per_domain: 10000

# Disable "seed scope" to allow crawl pages from any domain
link_storage.link_strategy.use_scope: false

# Set initial link classifier a simple one
link_storage.link_classifier.type: LinkClassifierBaseline

# Train a new link classifier while the crawler is running. This allows
# the crawler automatically learn how to prioritize links in order to
# efficiently locate relevant content while avoiding the retrieval of
# irrelevant content.
link_storage.online_learning.enabled: true
link_storage.online_learning.type: FORWARD_CLASSIFIER_BINARY
link_storage.online_learning.learning_limit: 1000

# Allways select top-k links with highest priority to be scheduled
link_storage.link_selector: TopkLinkSelector

# Configure the minimum time interval (in milliseconds) to wait between requests
# to the same host to avoid overloading servers. If you are crawling your own
# web site, you can descrease this value to speed-up the crawl.
link_storage.scheduler.host_min_access_interval: 5000

# Configure the User-Agent of the crawler
crawler_manager.downloader.user_agent.name: ACHE
crawler_manager.downloader.user_agent.url: https://github.com/ViDA-NYU/ache
